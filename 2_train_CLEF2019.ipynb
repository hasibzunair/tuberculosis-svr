{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the 3D-CNN on the SSS, ESS and SIZ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2507,
     "status": "ok",
     "timestamp": 1580190695724,
     "user": {
      "displayName": "Md Hasib Zunair 1320262643",
      "photoUrl": "",
      "userId": "12069756592370329757"
     },
     "user_tz": 300
    },
    "id": "tSs8XjcdteBW",
    "outputId": "ddf4bcee-cf2f-4633-b8a2-7293d5f1969f"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
    "from keras.layers import Dropout, Input, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adadelta, SGD\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#np.random.seed(1)\n",
    "\n",
    "# Print version\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"Tensorflow Version\", tf.__version__)\n",
    "\n",
    "# Helpers functions\n",
    "\n",
    "def create_directory(directory):\n",
    "    '''\n",
    "    Creates a new folder in the specified directory if the folder doesn't exist.\n",
    "    INPUT\n",
    "        directory: Folder to be created, called as \"folder/\".\n",
    "    OUTPUT\n",
    "        New folder in the current directory.\n",
    "    '''\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "# Define paths\n",
    "base_path = os.path.abspath(\"./\") # Your root directory\n",
    "dataset_path = os.path.join(base_path, \"dataset\") # Your dataset folder\n",
    "model_path = os.path.join(base_path, \"models\")\n",
    "log_path = os.path.join(base_path, \"logs\")\n",
    "\n",
    "# Name your experiment\n",
    "experiment_name = \"exp_siz\"\n",
    "\n",
    "create_directory(log_path)\n",
    "create_directory(log_path+\"/\"+experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4346,
     "status": "ok",
     "timestamp": 1580190697582,
     "user": {
      "displayName": "Md Hasib Zunair 1320262643",
      "photoUrl": "",
      "userId": "12069756592370329757"
     },
     "user_tz": 300
    },
    "id": "G4VLrAcoiKmV",
    "outputId": "3a24fbda-5486-446a-d494-b628c72ac965"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw = np.load('{}/x_train_siz_norm.npy'.format(dataset_path))\n",
    "labels = np.load('{}/y_train_clef.npy'.format(dataset_path))\n",
    "\n",
    "print(\"Raw data: \", raw.shape, labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw, labels, test_size=0.2, random_state=1)\n",
    "print(\"After splitting: \", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# If not cross validation, do this 60 20 20 split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "#print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "y = np.array([np.argmax(x) for x in y_train])\n",
    "print(\"Count: \", np.count_nonzero(y == 1), np.count_nonzero(y == 0))\n",
    "\n",
    "# For 60 20 20 split\n",
    "# train 65, 65\n",
    "# val 22, 22\n",
    "# test 20, 24\n",
    "\n",
    "def expand_dims(val):\n",
    "    val_exp = np.expand_dims(val, axis=4)\n",
    "    return val_exp\n",
    "\n",
    "X_train = expand_dims(X_train)\n",
    "#X_val = expand_dims(X_val)\n",
    "X_test = expand_dims(X_test)\n",
    "print(\"After expanding: \", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4827,
     "status": "ok",
     "timestamp": 1580190698104,
     "user": {
      "displayName": "Md Hasib Zunair 1320262643",
      "photoUrl": "",
      "userId": "12069756592370329757"
     },
     "user_tz": 300
    },
    "id": "R-O8vQnPtp4s",
    "outputId": "331c3a78-0f03-4bbc-e8e1-73a1dec79497"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "weights_path = \"{}/{}.h5\".format(log_path+\"/\"+experiment_name, \"best_model\")\n",
    "checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, monitor='val_loss', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-8, mode='auto') # new_lr = lr * factor\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, verbose=1, patience=8, mode='auto', restore_best_weights=True)\n",
    "csv_logger = CSVLogger('{}/training.csv'.format(log_path+\"/\"+experiment_name))\n",
    "\n",
    "# Define class weights for imbalacned data\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(np.argmax(y_train, axis=1)), np.argmax(y_train, axis=1))\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "def awesome_3D_network():\n",
    "    \n",
    "    input_layer = Input((128, 128, 64, 1))\n",
    "    \n",
    "    conv_layer1 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
    "    pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer1)\n",
    "\n",
    "    pooling_layer1 = BatchNormalization()(pooling_layer1)  \n",
    "    conv_layer2 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
    "    pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
    "    pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
    "    conv_layer3 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
    "    pooling_layer3 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer3)\n",
    "    pooling_layer3 = BatchNormalization()(pooling_layer3)\n",
    "    conv_layer4 = Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu')(pooling_layer3)\n",
    "    pooling_layer4 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
    "    pooling_layer4 = BatchNormalization()(pooling_layer4)\n",
    "    conv_layer5 = Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu')(pooling_layer4)\n",
    "    pooling_layer5 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer5)\n",
    "    \n",
    "    pooling_layer9 = BatchNormalization()(pooling_layer5)\n",
    "    flatten_layer = Flatten()(pooling_layer9)\n",
    "    \n",
    "    dense_layer3 = Dense(units=512, activation='relu')(flatten_layer)\n",
    "    dense_layer3 = Dropout(0.4)(dense_layer3)\n",
    "\n",
    "    dense_layer4 = Dense(units=256, activation='relu')(dense_layer3)\n",
    "    dense_layer4 = Dropout(0.4)(dense_layer3)\n",
    "  \n",
    "    output_layer = Dense(units=2, activation='softmax')(dense_layer4)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='mae', optimizer=SGD(lr=1e-06, momentum=0.99, decay=0.0, nesterov=False), metrics=['acc']) \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = None\n",
    "model = awesome_3D_network()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bI2F2Zw5-e2D",
    "outputId": "678ae6ec-c3db-4e85-f01c-2fcfcfa8c05f"
   },
   "outputs": [],
   "source": [
    "# kfold cross validation\n",
    "# https://www.kaggle.com/sharifamit19/data-augmentation-cross-validation-ensemble\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# define k-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "print(kfold)\n",
    "\n",
    "\n",
    "data = X_train\n",
    "# Flatten labels for making folds\n",
    "labels = np.array([np.argmax(x) for x in y_train])\n",
    "print(data.shape, labels.shape)\n",
    "\n",
    "\n",
    "# Store scores here\n",
    "cross_val_scores = []\n",
    "\n",
    "# Calculate the starting time    \n",
    "start_time = time.time()\n",
    "\n",
    "# Run folds\n",
    "\n",
    "for i, (train, test) in enumerate(kfold.split(data, labels)):\n",
    "    \n",
    "    print(\"Fold - {}\".format(i), data[train].shape, labels[train].shape, data[test].shape, labels[test].shape)\n",
    "    \n",
    "    # Clearing the NN.\n",
    "    K.clear_session()\n",
    "    model = None \n",
    "    \n",
    "    # Calculate the starting time    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create the model\n",
    "    model = awesome_3D_network()\n",
    "    \n",
    "    # One hot :convert class vectors to binary class matrices\n",
    "    y_train_cv = keras.utils.to_categorical(labels[train], 2)\n",
    "    y_val_cv = keras.utils.to_categorical(labels[test], 2)\n",
    "    \n",
    "    # Set callbacks\n",
    "    cb = None #[csv_logger, early_stopping, reduce_lr, checkpointer]\n",
    "    cw = None #class_weights\n",
    "\n",
    "    # Train\n",
    "    h=model.fit(x=data[train],     \n",
    "                y=y_train_cv,\n",
    "                validation_data=(data[test], y_val_cv), \n",
    "                batch_size=2, \n",
    "                epochs=100, \n",
    "                verbose=1,\n",
    "                class_weight = cw,\n",
    "                callbacks=cb,\n",
    "                shuffle=False,\n",
    "                )\n",
    "\n",
    "    # Evaluate\n",
    "\n",
    "    #score = model.evaluate(data[test], y_val_cv, verbose=0)\n",
    "    #loss, acc = score[0], score[1]\n",
    "\n",
    "    # Validation predictions stored here\n",
    "    y_pred_cv = []\n",
    "\n",
    "    # Iterate over images in validation\n",
    "    for img in data[test]:\n",
    "        img = np.expand_dims(img, axis=0)  # rank 4 tensor for prediction\n",
    "        y = model.predict(img)\n",
    "        y_pred_cv.append(y[:][0])\n",
    "\n",
    "    # Numpy\n",
    "    y_pred_cv = np.array(y_pred_cv)\n",
    "    \n",
    "    # Convert ground truth to column values\n",
    "    y_val_cv_flat = np.argmax(y_val_cv, axis=1)\n",
    "    # Get labels from predictions\n",
    "    y_pred_cv_flat = np.array([np.argmax(pred) for pred in y_pred_cv]) # y_pred[1] -> probability for class 1 \n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_val_cv_flat, y_pred_cv_flat) * 100\n",
    "    print(\"Fold {} accuracy :\".format(i), acc)\n",
    "\n",
    "    cross_val_scores.append(acc)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpGKr9eu-e0M"
   },
   "outputs": [],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cross_val_scores), np.std(cross_val_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vYDs1AhB4u8"
   },
   "outputs": [],
   "source": [
    "#SSS: 60.78% (+/- 13.34%)\n",
    "\n",
    "#ESS: 60.29% (+/- 9.25%)\n",
    "\n",
    "#SIZ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7KmWAqmtp-N"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "cb = None #[csv_logger, early_stopping, reduce_lr, checkpointer]\n",
    "cw = None #class_weights\n",
    "\n",
    "# Calculate the starting time    \n",
    "start_time = time.time()\n",
    "\n",
    "# One hot :convert class vectors to binary class matrices\n",
    "y_train_cv = keras.utils.to_categorical(y_train, 2)\n",
    "y_val_cv = keras.utils.to_categorical(y_val, 2)\n",
    "\n",
    "# Train\n",
    "h=model.fit(x=X_train,     \n",
    "            y=y_train,\n",
    "            validation_data=(X_val, y_val), \n",
    "            batch_size=2, \n",
    "            epochs=100, \n",
    "            verbose=1,\n",
    "            class_weight = cw,\n",
    "            callbacks=cb,\n",
    "            shuffle=False,\n",
    "            )\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bC3nKRDxUMm8"
   },
   "outputs": [],
   "source": [
    "# Plot and save accuravy loss graphs together\n",
    "def plot_loss_accu_all(history):\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'r')\n",
    "    plt.plot(epochs, val_acc, 'b')\n",
    "    plt.plot(epochs, loss, 'g')\n",
    "    plt.plot(epochs, val_loss, 'y')\n",
    "    plt.title('Accuracy/Loss graph')\n",
    "    \n",
    "    plt.ylabel('Performance Measure')\n",
    "    plt.xlabel('Epochs')\n",
    "    \n",
    "    plt.legend(['trainacc', 'valacc', 'trainloss', 'valloss'], loc='lower right', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('{}/{}.png'.format(log_path+\"/\"+experiment_name, \"trainval_acc_loss\"), dpi=500)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot and save accuravy loss graphs individually\n",
    "def plot_loss_accu(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'g')\n",
    "    plt.plot(epochs, val_loss, 'y')\n",
    "    #plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss %')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig('{}/{}_loss.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    #plt.savefig('{}/{}_loss.pdf'.format(output_path, EXP_NAME), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    loss = history.history['acc']\n",
    "    val_loss = history.history['val_acc']\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'r')\n",
    "    plt.plot(epochs, val_loss, 'b')\n",
    "    #plt.title('Training and validation accuracy')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig('{}/{}_acc.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    #plt.savefig('{}/{}_acc.pdf'.format(output_path, EXP_NAME), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_accu_all(model.history)\n",
    "print(\"Done training and logging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17qsdZzX3qcB"
   },
   "outputs": [],
   "source": [
    "model.save(\"{}/{}.h5\".format(log_path+\"/\"+experiment_name, experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGYJW-hwTUZw"
   },
   "outputs": [],
   "source": [
    "#model = None\n",
    "#model = load_model(\"{}/best_model.h5\".format(model_path))\n",
    "\n",
    "#score = model.evaluate(X_test, y_test, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YECSNf0vtTj"
   },
   "outputs": [],
   "source": [
    "# Inference on hold out test set\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for img in X_test:\n",
    "    img = np.expand_dims(img, axis=0)  # rank 4 tensor for prediction\n",
    "    y = model.predict(img)\n",
    "    y_pred.append(y[:][0])\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred.shape)\n",
    "\n",
    "# Convert ground truth to column values\n",
    "y_test_flat = np.argmax(y_test, axis=1)\n",
    "print(\"After flattening ground truth: \", y_test_flat.shape)\n",
    "\n",
    "# Get labels from predictions\n",
    "y_pred_flat = np.array([np.argmax(pred) for pred in y_pred]) # y_pred[1] -> probability for class 1 \n",
    "print(\"Binarize probability values: \", y_pred_flat.shape)\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "acc = accuracy_score(y_test_flat, y_pred_flat) * 100\n",
    "print(\"Accuracy :\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIZ: 63.63\n",
    "# SSS: 56.81\n",
    "# ESS: 61.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64nwLFXbw2Vm"
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test_flat, y_pred_flat) \n",
    "print(confusion_mtx)\n",
    "target_names = ['0', '1']\n",
    "print(classification_report(y_test_flat, y_pred_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEPVB-Qqw2Pe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Area under ROC curve : ', roc_auc_score(y_test, y_pred) *100 )\n",
    "\n",
    "# sss 62.5\n",
    "# ess 70.0\n",
    "# siz 67.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Hh61ucpx6_c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "cls = 1 # class name\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#print(roc_auc)\n",
    "print(\"Area under the ROC curve for positive class:\", roc_auc[1])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2 # line width\n",
    "plt.plot(fpr[cls], tpr[cls], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[cls])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('{}/{}.png'.format(log_path+\"/\"+experiment_name, \"roc\"), dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibMnW_4kbhpy"
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import pickle \n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('{}'.format(log_path+\"/\") + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('{}'.format(log_path+\"/\") + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "#https://github.com/hasibzunair/MelaNet/blob/master/isic2016_scripts/EDA.ipynb\n",
    "\n",
    "# Save AUCROC for plotting\n",
    "ascore = {}\n",
    "ascore[\"fpr\"] = fpr[cls]\n",
    "ascore[\"tpr\"] = tpr[cls]\n",
    "ascore[\"roc_auc\"] = roc_auc[cls]\n",
    "save_obj(ascore, experiment_name)\n",
    "\n",
    "type(fpr[cls]), roc_auc[cls] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TL4iR6rdVox0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ut7rFRxVor8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCt7UgGMVoqE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crSuFNoYVonr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqN510l6TUgK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsBAJmom31Ep"
   },
   "source": [
    "### Inference on ImageCLEF test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Syi4HYH73qfX"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model = load_model(\"{}/best_model.h5\".format(model_path))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F60ZqaHO3qnP"
   },
   "outputs": [],
   "source": [
    "x_test = expand_dims(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlYy17EB3qtG"
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in x_test:\n",
    "    i = np.expand_dims(i, axis=0)\n",
    "    y_pred = model.predict(i)\n",
    "    res.append(y_pred)\n",
    "    #print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2uoQDJK3qq9"
   },
   "outputs": [],
   "source": [
    "res = np.array(res)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWtc_No1iI7p"
   },
   "outputs": [],
   "source": [
    "res[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95h2C3JlidGr"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcoXaust3qkz"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "dt = pd.read_csv('{}/TestSet_metaData.csv'.format(dataset_path))\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPBhkjsM3qiy"
   },
   "outputs": [],
   "source": [
    "patient_names = dt['Filename'].values\n",
    "len(patient_names)\n",
    "\n",
    "names = []\n",
    "\n",
    "for name in patient_names:\n",
    "    names.append(name[:-7])\n",
    "\n",
    "names[:5]\n",
    "\n",
    "probab = []\n",
    "\n",
    "for p in res:\n",
    "    # probability of HIGH severity as required to make submission\n",
    "    probab.append(p[0][1])\n",
    "    \n",
    "probab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7oyQ2uMD3qY-"
   },
   "outputs": [],
   "source": [
    "for n, p in zip(names, probab):\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qE_cR4n4tqMn"
   },
   "outputs": [],
   "source": [
    "with open('{}/submission.txt'.format(dataset_path), 'w') as f:\n",
    "    for n, p in zip(names, probab):\n",
    "        print(n,\",\", p)\n",
    "        f.write(str(n))\n",
    "        f.write(\",\")\n",
    "        f.write(str(p))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMDeNXJx1TxkuBCBZX5hqeg",
   "collapsed_sections": [],
   "name": "train_clef19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
