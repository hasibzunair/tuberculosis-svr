{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the 3D CNN on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2507,
     "status": "ok",
     "timestamp": 1580190695724,
     "user": {
      "displayName": "Md Hasib Zunair 1320262643",
      "photoUrl": "",
      "userId": "12069756592370329757"
     },
     "user_tz": 300
    },
    "id": "tSs8XjcdteBW",
    "outputId": "ddf4bcee-cf2f-4633-b8a2-7293d5f1969f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version 2.3.1\n",
      "Tensorflow Version 2.0.0\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
    "from keras.layers import Dropout, Input, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adadelta, SGD\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#np.random.seed(1)\n",
    "\n",
    "# Print version\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"Tensorflow Version\", tf.__version__)\n",
    "\n",
    "# Helpers functions\n",
    "\n",
    "def create_directory(directory):\n",
    "    '''\n",
    "    Creates a new folder in the specified directory if the folder doesn't exist.\n",
    "    INPUT\n",
    "        directory: Folder to be created, called as \"folder/\".\n",
    "    OUTPUT\n",
    "        New folder in the current directory.\n",
    "    '''\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "# Define paths\n",
    "base_path = os.path.abspath(\"./\") # Your root directory\n",
    "dataset_path = os.path.join(base_path, \"dataset\") # Your dataset folder\n",
    "model_path = os.path.join(base_path, \"models\")\n",
    "log_path = os.path.join(base_path, \"logs\")\n",
    "\n",
    "# Name your experiment\n",
    "experiment_name = \"sss\"\n",
    "\n",
    "create_directory(log_path)\n",
    "create_directory(log_path+\"/\"+experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4346,
     "status": "ok",
     "timestamp": 1580190697582,
     "user": {
      "displayName": "Md Hasib Zunair 1320262643",
      "photoUrl": "",
      "userId": "12069756592370329757"
     },
     "user_tz": 300
    },
    "id": "G4VLrAcoiKmV",
    "outputId": "3a24fbda-5486-446a-d494-b628c72ac965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:  (218, 128, 128, 64) (218, 2)\n",
      "After splitting:  (174, 128, 128, 64) (174, 2) (44, 128, 128, 64) (44, 2)\n",
      "Count:  87 87\n",
      "After expanding:  (174, 128, 128, 64, 1) (44, 128, 128, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw = np.load('{}/x_train_sss_norm.npy'.format(dataset_path))\n",
    "labels = np.load('{}/y_train_clef.npy'.format(dataset_path))\n",
    "\n",
    "print(\"Raw data: \", raw.shape, labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw, labels, test_size=0.2, random_state=1)\n",
    "print(\"After splitting: \", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# If not cross validation, do this 60 20 20 split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "#print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "y = np.array([np.argmax(x) for x in y_train])\n",
    "print(\"Count: \", np.count_nonzero(y == 1), np.count_nonzero(y == 0))\n",
    "\n",
    "# For 60 20 20 split\n",
    "# train 65, 65\n",
    "# val 22, 22\n",
    "# test 20, 24\n",
    "\n",
    "\n",
    "def expand_dims(val):\n",
    "    val_exp = np.expand_dims(val, axis=4)\n",
    "    return val_exp\n",
    "\n",
    "X_train = expand_dims(X_train)\n",
    "#X_val = expand_dims(X_val)\n",
    "X_test = expand_dims(X_test)\n",
    "print(\"After expanding: \", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4827,
     "status": "ok",
     "timestamp": 1580190698104,
     "user": {
      "displayName": "Md Hasib Zunair 1320262643",
      "photoUrl": "",
      "userId": "12069756592370329757"
     },
     "user_tz": 300
    },
    "id": "R-O8vQnPtp4s",
    "outputId": "331c3a78-0f03-4bbc-e8e1-73a1dec79497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1.]\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 64, 1)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 126, 126, 62, 64)  1792      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 63, 63, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 63, 63, 31, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 61, 61, 29, 64)    110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 30, 30, 14, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 30, 14, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 28, 28, 12, 128)   221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 14, 14, 6, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 6, 128)    512       \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 12, 12, 4, 256)    884992    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 6, 6, 2, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 2, 256)      1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               9437696   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 10,659,522\n",
      "Trainable params: 10,658,498\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "weights_path = \"{}/{}.h5\".format(log_path+\"/\"+experiment_name, \"best_model\")\n",
    "checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, monitor='val_loss', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-8, mode='auto') # new_lr = lr * factor\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, verbose=1, patience=8, mode='auto', restore_best_weights=True)\n",
    "csv_logger = CSVLogger('{}/training.csv'.format(log_path+\"/\"+experiment_name))\n",
    "\n",
    "# Define class weights for imbalacned data\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(np.argmax(y_train, axis=1)), np.argmax(y_train, axis=1))\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "def awesome_3D_network():\n",
    "    \n",
    "    filter_size = 32\n",
    "    depth = 64\n",
    "    input_layer = Input((128, 128, depth, 1)) # 1 is just dummy dimension good for nothing \n",
    "    \n",
    "    conv_layer1 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
    "    pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer1)\n",
    "\n",
    "    pooling_layer1 = BatchNormalization()(pooling_layer1)  \n",
    "    conv_layer2 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
    "    pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
    "    pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
    "    conv_layer3 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
    "    pooling_layer3 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer3)\n",
    "    pooling_layer3 = BatchNormalization()(pooling_layer3)\n",
    "    conv_layer4 = Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu')(pooling_layer3)\n",
    "    pooling_layer4 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
    "    pooling_layer4 = BatchNormalization()(pooling_layer4)\n",
    "    conv_layer5 = Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu')(pooling_layer4)\n",
    "    pooling_layer5 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer5)\n",
    "    \n",
    "    pooling_layer9 = BatchNormalization()(pooling_layer5)\n",
    "    flatten_layer = Flatten()(pooling_layer9)\n",
    "\n",
    "    #dense_layer1 = Dense(units=1028, activation='relu')(flatten_layer)\n",
    "    #dense_layer1 = Dropout(0.4)(dense_layer1)\n",
    "    \n",
    "    #dense_layer2 = Dense(units=1028, activation='relu')(flatten_layer)\n",
    "    #dense_layer2 = Dropout(0.4)(dense_layer2)\n",
    "    \n",
    "    dense_layer3 = Dense(units=512, activation='relu')(flatten_layer)\n",
    "    dense_layer3 = Dropout(0.4)(dense_layer3)\n",
    "\n",
    "    dense_layer4 = Dense(units=256, activation='relu')(dense_layer3)\n",
    "    dense_layer4 = Dropout(0.4)(dense_layer3)\n",
    "  \n",
    "    output_layer = Dense(units=2, activation='softmax')(dense_layer4)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='mae', optimizer=SGD(lr=1e-06, momentum=0.99, decay=0.0, nesterov=False), metrics=['acc']) \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = None\n",
    "model = awesome_3D_network()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bI2F2Zw5-e2D",
    "outputId": "678ae6ec-c3db-4e85-f01c-2fcfcfa8c05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
      "(174, 128, 128, 64, 1) (174,)\n",
      "Fold - 0 (156, 128, 128, 64, 1) (156,) (18, 128, 128, 64, 1) (18,)\n",
      "Train on 156 samples, validate on 18 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 9s 55ms/step - loss: 0.5017 - acc: 0.4872 - val_loss: 0.4972 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 6s 37ms/step - loss: 0.4818 - acc: 0.5064 - val_loss: 0.4942 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.4637 - acc: 0.5192 - val_loss: 0.4866 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.4060 - acc: 0.5962 - val_loss: 0.4743 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3404 - acc: 0.6667 - val_loss: 0.4909 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.2951 - acc: 0.7436 - val_loss: 0.4983 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2638 - acc: 0.7628 - val_loss: 0.5007 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.2600 - acc: 0.7564 - val_loss: 0.5001 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.2679 - acc: 0.7756 - val_loss: 0.4972 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.2228 - acc: 0.8013 - val_loss: 0.5064 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.2128 - acc: 0.8205 - val_loss: 0.5943 - val_acc: 0.3889\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.1816 - acc: 0.8333 - val_loss: 0.4905 - val_acc: 0.5556\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1542 - acc: 0.8846 - val_loss: 0.5214 - val_acc: 0.5556\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1253 - acc: 0.9038 - val_loss: 0.6015 - val_acc: 0.3333\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.1281 - acc: 0.9038 - val_loss: 0.5949 - val_acc: 0.3333\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0869 - acc: 0.9423 - val_loss: 0.5161 - val_acc: 0.5556\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1260 - acc: 0.9103 - val_loss: 0.5625 - val_acc: 0.3889\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0718 - acc: 0.9679 - val_loss: 0.6064 - val_acc: 0.3889\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0789 - acc: 0.9551 - val_loss: 0.6142 - val_acc: 0.3889\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0485 - acc: 0.9808 - val_loss: 0.5842 - val_acc: 0.3889\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0422 - acc: 0.9936 - val_loss: 0.5719 - val_acc: 0.4444\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0685 - acc: 0.9551 - val_loss: 0.5880 - val_acc: 0.3333\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0440 - acc: 0.9872 - val_loss: 0.6034 - val_acc: 0.2778\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0617 - acc: 0.9744 - val_loss: 0.5918 - val_acc: 0.3889\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0617 - acc: 0.9615 - val_loss: 0.5865 - val_acc: 0.3889\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0339 - acc: 0.9936 - val_loss: 0.5896 - val_acc: 0.3333\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0319 - acc: 0.9808 - val_loss: 0.5665 - val_acc: 0.4444\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0247 - acc: 0.9872 - val_loss: 0.5571 - val_acc: 0.4444\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0375 - acc: 0.9872 - val_loss: 0.5572 - val_acc: 0.5000\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0389 - acc: 0.9744 - val_loss: 0.5443 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0338 - acc: 0.9936 - val_loss: 0.5664 - val_acc: 0.4444\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0303 - acc: 0.9936 - val_loss: 0.5979 - val_acc: 0.3333\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0361 - acc: 0.9808 - val_loss: 0.5932 - val_acc: 0.2778\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0246 - acc: 0.9936 - val_loss: 0.5723 - val_acc: 0.4444\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0163 - acc: 0.9936 - val_loss: 0.5601 - val_acc: 0.5000\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.5676 - val_acc: 0.4444\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0192 - acc: 0.9936 - val_loss: 0.5578 - val_acc: 0.5000\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.5544 - val_acc: 0.5000\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.5567 - val_acc: 0.5000\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.5697 - val_acc: 0.3333\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0192 - acc: 0.9936 - val_loss: 0.5653 - val_acc: 0.3889\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.5586 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.5519 - val_acc: 0.5000\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.5634 - val_acc: 0.4444\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.5656 - val_acc: 0.4444\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.5563 - val_acc: 0.4444\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.5527 - val_acc: 0.5000\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0139 - acc: 0.9872 - val_loss: 0.5498 - val_acc: 0.5000\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.5474 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.5632 - val_acc: 0.4444\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0203 - acc: 0.9872 - val_loss: 0.5796 - val_acc: 0.3333\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.5723 - val_acc: 0.3333\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.5705 - val_acc: 0.3333\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.5717 - val_acc: 0.3333\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.5794 - val_acc: 0.3333\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0175 - acc: 0.9936 - val_loss: 0.5654 - val_acc: 0.3889\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.5489 - val_acc: 0.5000\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.5549 - val_acc: 0.5000\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0145 - acc: 0.9936 - val_loss: 0.5614 - val_acc: 0.3889\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.5723 - val_acc: 0.3333\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.5708 - val_acc: 0.3333\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.5582 - val_acc: 0.5000\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.5626 - val_acc: 0.4444\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0129 - acc: 0.9936 - val_loss: 0.5677 - val_acc: 0.4444\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.5632 - val_acc: 0.5000\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.5728 - val_acc: 0.3333\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.5824 - val_acc: 0.3333\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.5709 - val_acc: 0.3333\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.5537 - val_acc: 0.4444\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.5560 - val_acc: 0.4444\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.5642 - val_acc: 0.4444\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0101 - acc: 0.9936 - val_loss: 0.5761 - val_acc: 0.3889\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0108 - acc: 0.9936 - val_loss: 0.5909 - val_acc: 0.3333\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.6056 - val_acc: 0.3333\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.6066 - val_acc: 0.3333\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0137 - acc: 0.9936 - val_loss: 0.5949 - val_acc: 0.3333\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0139 - acc: 0.9936 - val_loss: 0.5832 - val_acc: 0.3333\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.5793 - val_acc: 0.3333\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.5812 - val_acc: 0.3889\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.5726 - val_acc: 0.4444\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.5591 - val_acc: 0.4444\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.5614 - val_acc: 0.4444\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.5736 - val_acc: 0.3889\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.5825 - val_acc: 0.3889\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.5785 - val_acc: 0.3889\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.5794 - val_acc: 0.3889\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5842 - val_acc: 0.3889\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.5889 - val_acc: 0.3333\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.5898 - val_acc: 0.3333\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.5876 - val_acc: 0.3333\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.5851 - val_acc: 0.3333\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.5824 - val_acc: 0.3333\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.5872 - val_acc: 0.3333\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.5864 - val_acc: 0.3333\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 6s 37ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.5845 - val_acc: 0.3333\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.5806 - val_acc: 0.3333\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0112 - acc: 0.9936 - val_loss: 0.5769 - val_acc: 0.3889\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5747 - val_acc: 0.3889\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.5789 - val_acc: 0.3889\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.5846 - val_acc: 0.3333\n",
      "Fold 0 accuracy : 33.33333333333333\n",
      "Fold - 1 (156, 128, 128, 64, 1) (156,) (18, 128, 128, 64, 1) (18,)\n",
      "Train on 156 samples, validate on 18 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 7s 46ms/step - loss: 0.5025 - acc: 0.4872 - val_loss: 0.5006 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.4787 - acc: 0.5064 - val_loss: 0.5013 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.4573 - acc: 0.5321 - val_loss: 0.5019 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3914 - acc: 0.6346 - val_loss: 0.5054 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3837 - acc: 0.6410 - val_loss: 0.5323 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3117 - acc: 0.7436 - val_loss: 0.5587 - val_acc: 0.3333\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2759 - acc: 0.7500 - val_loss: 0.5154 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2520 - acc: 0.7692 - val_loss: 0.4773 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2713 - acc: 0.7628 - val_loss: 0.4874 - val_acc: 0.4444\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2252 - acc: 0.8269 - val_loss: 0.4812 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2050 - acc: 0.8397 - val_loss: 0.4446 - val_acc: 0.6111\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1613 - acc: 0.8910 - val_loss: 0.4423 - val_acc: 0.5556\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1521 - acc: 0.8846 - val_loss: 0.4401 - val_acc: 0.6111\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1545 - acc: 0.8590 - val_loss: 0.4420 - val_acc: 0.5556\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1270 - acc: 0.9295 - val_loss: 0.4387 - val_acc: 0.5556\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0891 - acc: 0.9615 - val_loss: 0.4457 - val_acc: 0.6667\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.1036 - acc: 0.9423 - val_loss: 0.4469 - val_acc: 0.6667\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1042 - acc: 0.9295 - val_loss: 0.4313 - val_acc: 0.6667\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0628 - acc: 0.9744 - val_loss: 0.4320 - val_acc: 0.6111\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0417 - acc: 0.9872 - val_loss: 0.4302 - val_acc: 0.6667\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0767 - acc: 0.9551 - val_loss: 0.4319 - val_acc: 0.6667\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0662 - acc: 0.9744 - val_loss: 0.4255 - val_acc: 0.6667\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0626 - acc: 0.9744 - val_loss: 0.4290 - val_acc: 0.6111\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0552 - acc: 0.9744 - val_loss: 0.4333 - val_acc: 0.6111\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0626 - acc: 0.9679 - val_loss: 0.4474 - val_acc: 0.5556\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0457 - acc: 0.9744 - val_loss: 0.4535 - val_acc: 0.5556\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0399 - acc: 0.9808 - val_loss: 0.4598 - val_acc: 0.5556\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 6s 37ms/step - loss: 0.0421 - acc: 0.9872 - val_loss: 0.4585 - val_acc: 0.5000\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0408 - acc: 0.9808 - val_loss: 0.4456 - val_acc: 0.5556\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0285 - acc: 1.0000 - val_loss: 0.4380 - val_acc: 0.6111\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0360 - acc: 0.9936 - val_loss: 0.4306 - val_acc: 0.6111\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0312 - acc: 0.9936 - val_loss: 0.4314 - val_acc: 0.6111\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0258 - acc: 0.9936 - val_loss: 0.4315 - val_acc: 0.6111\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0305 - acc: 0.9936 - val_loss: 0.4344 - val_acc: 0.6111\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0397 - acc: 0.9808 - val_loss: 0.4318 - val_acc: 0.5556\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.4249 - val_acc: 0.5556\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0208 - acc: 0.9936 - val_loss: 0.4221 - val_acc: 0.6111\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.4215 - val_acc: 0.6111\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0248 - acc: 0.9936 - val_loss: 0.4220 - val_acc: 0.6111\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0276 - acc: 0.9936 - val_loss: 0.4258 - val_acc: 0.6111\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.4254 - val_acc: 0.6111\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.4250 - val_acc: 0.6111\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0188 - acc: 0.9936 - val_loss: 0.4188 - val_acc: 0.6111\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.4123 - val_acc: 0.6111\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 6s 42ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.4126 - val_acc: 0.6111\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.4183 - val_acc: 0.6111\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.4201 - val_acc: 0.6111\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.4198 - val_acc: 0.6111\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.4201 - val_acc: 0.6111\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0280 - acc: 0.9936 - val_loss: 0.4156 - val_acc: 0.6111\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.4146 - val_acc: 0.6111\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.4180 - val_acc: 0.6111\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0178 - acc: 0.9872 - val_loss: 0.4225 - val_acc: 0.6111\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.4242 - val_acc: 0.5556\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0164 - acc: 0.9936 - val_loss: 0.4250 - val_acc: 0.6111\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0166 - acc: 0.9936 - val_loss: 0.4262 - val_acc: 0.6111\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0232 - acc: 0.9936 - val_loss: 0.4235 - val_acc: 0.6111\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0230 - acc: 0.9872 - val_loss: 0.4200 - val_acc: 0.5556\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.4196 - val_acc: 0.5556\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0225 - acc: 0.9936 - val_loss: 0.4176 - val_acc: 0.6111\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.4193 - val_acc: 0.6111\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.4200 - val_acc: 0.6111\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.4202 - val_acc: 0.6111\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.4186 - val_acc: 0.6111\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.4149 - val_acc: 0.6111\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.4138 - val_acc: 0.6111\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.4136 - val_acc: 0.6111\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.4172 - val_acc: 0.6111\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.4177 - val_acc: 0.6111\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.4197 - val_acc: 0.6111\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.4212 - val_acc: 0.6111\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.4174 - val_acc: 0.6111\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.4170 - val_acc: 0.6111\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.4175 - val_acc: 0.6111\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.4171 - val_acc: 0.6111\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0104 - acc: 0.9936 - val_loss: 0.4180 - val_acc: 0.6111\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0102 - acc: 0.9936 - val_loss: 0.4181 - val_acc: 0.6111\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.4176 - val_acc: 0.5556\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.4179 - val_acc: 0.5556\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 6s 42ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.4161 - val_acc: 0.6111\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0138 - acc: 0.9936 - val_loss: 0.4162 - val_acc: 0.6111\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.4152 - val_acc: 0.6111\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.4149 - val_acc: 0.6111\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.4168 - val_acc: 0.6111\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.4187 - val_acc: 0.6111\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.4182 - val_acc: 0.6111\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.4175 - val_acc: 0.6111\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.4151 - val_acc: 0.6111\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0098 - acc: 0.9936 - val_loss: 0.4174 - val_acc: 0.5556\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.4170 - val_acc: 0.5556\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.4169 - val_acc: 0.5556\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0128 - acc: 0.9936 - val_loss: 0.4151 - val_acc: 0.5556\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.4153 - val_acc: 0.5556\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.4180 - val_acc: 0.5556\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.4186 - val_acc: 0.5556\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.4166 - val_acc: 0.5556\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.4143 - val_acc: 0.6111\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.4125 - val_acc: 0.6111\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.4108 - val_acc: 0.6111\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.4102 - val_acc: 0.6111\n",
      "Fold 1 accuracy : 61.111111111111114\n",
      "Fold - 2 (156, 128, 128, 64, 1) (156,) (18, 128, 128, 64, 1) (18,)\n",
      "Train on 156 samples, validate on 18 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.5455 - acc: 0.4679 - val_loss: 0.5002 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.5079 - acc: 0.5000 - val_loss: 0.5004 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.4106 - acc: 0.5833 - val_loss: 0.5033 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.4232 - acc: 0.6026 - val_loss: 0.5062 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3725 - acc: 0.6346 - val_loss: 0.5037 - val_acc: 0.5556\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3112 - acc: 0.6987 - val_loss: 0.5030 - val_acc: 0.4444\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3200 - acc: 0.7051 - val_loss: 0.4834 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2191 - acc: 0.8013 - val_loss: 0.4527 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1989 - acc: 0.8526 - val_loss: 0.4140 - val_acc: 0.6111\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1853 - acc: 0.8462 - val_loss: 0.3409 - val_acc: 0.6667\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1806 - acc: 0.8590 - val_loss: 0.2825 - val_acc: 0.8889\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1441 - acc: 0.8782 - val_loss: 0.3068 - val_acc: 0.6667\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1531 - acc: 0.8718 - val_loss: 0.2729 - val_acc: 0.8333\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1160 - acc: 0.9038 - val_loss: 0.2635 - val_acc: 0.8333\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1039 - acc: 0.9295 - val_loss: 0.2777 - val_acc: 0.8889\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1088 - acc: 0.9103 - val_loss: 0.2916 - val_acc: 0.7778\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0938 - acc: 0.9359 - val_loss: 0.3170 - val_acc: 0.7778\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0766 - acc: 0.9551 - val_loss: 0.2966 - val_acc: 0.8333\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0921 - acc: 0.9359 - val_loss: 0.2892 - val_acc: 0.7778\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0752 - acc: 0.9487 - val_loss: 0.2719 - val_acc: 0.8889\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0685 - acc: 0.9615 - val_loss: 0.2881 - val_acc: 0.7778\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0682 - acc: 0.9551 - val_loss: 0.2815 - val_acc: 0.8333\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0542 - acc: 0.9808 - val_loss: 0.2858 - val_acc: 0.8333\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0700 - acc: 0.9615 - val_loss: 0.2729 - val_acc: 0.8333\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0500 - acc: 0.9744 - val_loss: 0.2603 - val_acc: 0.8889\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0507 - acc: 0.9744 - val_loss: 0.2615 - val_acc: 0.8333\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0404 - acc: 0.9808 - val_loss: 0.2610 - val_acc: 0.8333\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0379 - acc: 0.9872 - val_loss: 0.2718 - val_acc: 0.8333\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0408 - acc: 0.9808 - val_loss: 0.2691 - val_acc: 0.8333\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0329 - acc: 0.9872 - val_loss: 0.2789 - val_acc: 0.7778\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0444 - acc: 0.9744 - val_loss: 0.2605 - val_acc: 0.8333\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0325 - acc: 0.9936 - val_loss: 0.2586 - val_acc: 0.8333\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0300 - acc: 0.9872 - val_loss: 0.2612 - val_acc: 0.8889\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0362 - acc: 0.9872 - val_loss: 0.2707 - val_acc: 0.8333\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0368 - acc: 0.9744 - val_loss: 0.2736 - val_acc: 0.7778\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0365 - acc: 0.9808 - val_loss: 0.2719 - val_acc: 0.7778\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0211 - acc: 0.9936 - val_loss: 0.2690 - val_acc: 0.8333\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0255 - acc: 0.9872 - val_loss: 0.2614 - val_acc: 0.8333\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0308 - acc: 0.9808 - val_loss: 0.2594 - val_acc: 0.7778\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0238 - acc: 0.9872 - val_loss: 0.2656 - val_acc: 0.7778\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0245 - acc: 0.9936 - val_loss: 0.2678 - val_acc: 0.7778\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0242 - acc: 0.9872 - val_loss: 0.2692 - val_acc: 0.7778\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0247 - acc: 0.9936 - val_loss: 0.2611 - val_acc: 0.7778\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.2486 - val_acc: 0.8333\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0246 - acc: 0.9872 - val_loss: 0.2455 - val_acc: 0.8889\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0197 - acc: 0.9936 - val_loss: 0.2508 - val_acc: 0.8333\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0129 - acc: 0.9936 - val_loss: 0.2607 - val_acc: 0.8333\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0137 - acc: 0.9936 - val_loss: 0.2653 - val_acc: 0.7778\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0204 - acc: 0.9872 - val_loss: 0.2553 - val_acc: 0.7778\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0088 - acc: 0.9936 - val_loss: 0.2495 - val_acc: 0.7778\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0196 - acc: 0.9936 - val_loss: 0.2475 - val_acc: 0.7778\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.2499 - val_acc: 0.7778\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0138 - acc: 0.9936 - val_loss: 0.2496 - val_acc: 0.7778\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0166 - acc: 0.9936 - val_loss: 0.2544 - val_acc: 0.7778\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.2534 - val_acc: 0.7778\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.2529 - val_acc: 0.7778\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.2445 - val_acc: 0.7778\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.2336 - val_acc: 0.8333\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.2307 - val_acc: 0.8333\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.2300 - val_acc: 0.8889\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.2296 - val_acc: 0.8889\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.2343 - val_acc: 0.7778\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.2399 - val_acc: 0.7778\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.2418 - val_acc: 0.7778\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.2412 - val_acc: 0.8333\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0130 - acc: 0.9936 - val_loss: 0.2399 - val_acc: 0.8333\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.2400 - val_acc: 0.8333\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.2383 - val_acc: 0.8333\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0099 - acc: 0.9936 - val_loss: 0.2377 - val_acc: 0.8333\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.2393 - val_acc: 0.8333\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.2360 - val_acc: 0.8333\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.2355 - val_acc: 0.7778\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.2353 - val_acc: 0.7778\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.2376 - val_acc: 0.7778\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0144 - acc: 0.9936 - val_loss: 0.2388 - val_acc: 0.8333\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.2387 - val_acc: 0.7778\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2366 - val_acc: 0.7778\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.2332 - val_acc: 0.8333\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2306 - val_acc: 0.8333\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2321 - val_acc: 0.7778\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2362 - val_acc: 0.7778\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.2427 - val_acc: 0.8333\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.2442 - val_acc: 0.7778\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.2447 - val_acc: 0.7778\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.2401 - val_acc: 0.7778\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2372 - val_acc: 0.7778\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.2408 - val_acc: 0.8333\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.2443 - val_acc: 0.8333\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2488 - val_acc: 0.8333\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.2420 - val_acc: 0.8333\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0084 - acc: 0.9936 - val_loss: 0.2342 - val_acc: 0.7778\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0096 - acc: 0.9936 - val_loss: 0.2314 - val_acc: 0.7778\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2316 - val_acc: 0.8333\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.2327 - val_acc: 0.8333\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.2376 - val_acc: 0.7778\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.2433 - val_acc: 0.7778\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2466 - val_acc: 0.7778\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2493 - val_acc: 0.8333\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.2496 - val_acc: 0.8333\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.2492 - val_acc: 0.7778\n",
      "Fold 2 accuracy : 77.77777777777779\n",
      "Fold - 3 (156, 128, 128, 64, 1) (156,) (18, 128, 128, 64, 1) (18,)\n",
      "Train on 156 samples, validate on 18 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.5295 - acc: 0.4744 - val_loss: 0.5003 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.4250 - acc: 0.5833 - val_loss: 0.5002 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3944 - acc: 0.6346 - val_loss: 0.4914 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.3393 - acc: 0.6923 - val_loss: 0.4781 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.2851 - acc: 0.7500 - val_loss: 0.4598 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.2705 - acc: 0.7885 - val_loss: 0.4371 - val_acc: 0.7222\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.2343 - acc: 0.8077 - val_loss: 0.4177 - val_acc: 0.6111\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.2241 - acc: 0.8077 - val_loss: 0.4074 - val_acc: 0.6667\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.1895 - acc: 0.8462 - val_loss: 0.3873 - val_acc: 0.6111\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1521 - acc: 0.9103 - val_loss: 0.3816 - val_acc: 0.6111\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1507 - acc: 0.8782 - val_loss: 0.3683 - val_acc: 0.6667\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.1377 - acc: 0.8974 - val_loss: 0.3642 - val_acc: 0.6667\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.1336 - acc: 0.9167 - val_loss: 0.3677 - val_acc: 0.6667\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.1144 - acc: 0.9423 - val_loss: 0.3660 - val_acc: 0.6667\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0964 - acc: 0.9487 - val_loss: 0.3604 - val_acc: 0.6667\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0987 - acc: 0.9487 - val_loss: 0.3618 - val_acc: 0.6111\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0794 - acc: 0.9487 - val_loss: 0.3723 - val_acc: 0.6111\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0616 - acc: 0.9808 - val_loss: 0.3729 - val_acc: 0.6111\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0524 - acc: 0.9936 - val_loss: 0.3771 - val_acc: 0.6667\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0620 - acc: 0.9808 - val_loss: 0.3713 - val_acc: 0.7222\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0709 - acc: 0.9808 - val_loss: 0.3609 - val_acc: 0.6667\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0465 - acc: 0.9679 - val_loss: 0.3448 - val_acc: 0.7222\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0291 - acc: 0.9936 - val_loss: 0.3423 - val_acc: 0.6667\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0392 - acc: 0.9872 - val_loss: 0.3518 - val_acc: 0.6111\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0289 - acc: 0.9936 - val_loss: 0.3661 - val_acc: 0.6111\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0386 - acc: 0.9872 - val_loss: 0.3697 - val_acc: 0.6111\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0259 - acc: 0.9936 - val_loss: 0.3647 - val_acc: 0.6667\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0384 - acc: 0.9936 - val_loss: 0.3484 - val_acc: 0.7222\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0270 - acc: 0.9872 - val_loss: 0.3473 - val_acc: 0.6667\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0319 - acc: 0.9936 - val_loss: 0.3507 - val_acc: 0.6667\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.3538 - val_acc: 0.6111\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.3575 - val_acc: 0.6111\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0361 - acc: 0.9872 - val_loss: 0.3561 - val_acc: 0.6111\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0365 - acc: 0.9744 - val_loss: 0.3469 - val_acc: 0.6667\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.3440 - val_acc: 0.6667\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0211 - acc: 0.9936 - val_loss: 0.3455 - val_acc: 0.7222\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.3465 - val_acc: 0.6667\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.3455 - val_acc: 0.7222\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0189 - acc: 0.9936 - val_loss: 0.3455 - val_acc: 0.7222\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0219 - acc: 0.9872 - val_loss: 0.3418 - val_acc: 0.6667\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.3395 - val_acc: 0.6667\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.3397 - val_acc: 0.6667\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.3419 - val_acc: 0.6667\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.3415 - val_acc: 0.6667\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.3393 - val_acc: 0.7222\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0187 - acc: 0.9936 - val_loss: 0.3391 - val_acc: 0.6667\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 6s 38ms/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.3370 - val_acc: 0.6667\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0136 - acc: 0.9936 - val_loss: 0.3331 - val_acc: 0.6667\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.3332 - val_acc: 0.6667\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0201 - acc: 0.9872 - val_loss: 0.3354 - val_acc: 0.6667\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0178 - acc: 0.9936 - val_loss: 0.3405 - val_acc: 0.6667\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 6s 39ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.3408 - val_acc: 0.6667\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.3383 - val_acc: 0.6667\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.3371 - val_acc: 0.6667\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 6s 40ms/step - loss: 0.0120 - acc: 0.9936 - val_loss: 0.3395 - val_acc: 0.6667\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 6s 41ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.3407 - val_acc: 0.6667\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.3410 - val_acc: 0.6667\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.3435 - val_acc: 0.6667\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.3415 - val_acc: 0.6667\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.3431 - val_acc: 0.7222\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.3420 - val_acc: 0.6667\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.3431 - val_acc: 0.6667\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.3436 - val_acc: 0.6667\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.3426 - val_acc: 0.6667\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.3393 - val_acc: 0.6667\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.3362 - val_acc: 0.6667\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.3344 - val_acc: 0.6667\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.3366 - val_acc: 0.7222\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.3398 - val_acc: 0.6667\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.3399 - val_acc: 0.6667\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.3399 - val_acc: 0.6667\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.3377 - val_acc: 0.6667\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.3344 - val_acc: 0.6667\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.3351 - val_acc: 0.6667\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.3359 - val_acc: 0.6667\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.3373 - val_acc: 0.6667\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.3365 - val_acc: 0.6667\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.3366 - val_acc: 0.6667\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.3375 - val_acc: 0.6667\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.3367 - val_acc: 0.6667\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.3369 - val_acc: 0.6667\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.3373 - val_acc: 0.6667\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.3353 - val_acc: 0.6667\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.3343 - val_acc: 0.6667\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.3303 - val_acc: 0.6667\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 6s 42ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.3309 - val_acc: 0.6667\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3315 - val_acc: 0.6667\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.3331 - val_acc: 0.6667\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.3334 - val_acc: 0.6667\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.3336 - val_acc: 0.6667\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.3341 - val_acc: 0.6667\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.3332 - val_acc: 0.6667\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 7s 43ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3309 - val_acc: 0.6667\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.3291 - val_acc: 0.6667\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.3299 - val_acc: 0.6667\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.3322 - val_acc: 0.6667\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.3321 - val_acc: 0.6667\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.3300 - val_acc: 0.6667\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3286 - val_acc: 0.6667\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 7s 42ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.3268 - val_acc: 0.6667\n",
      "Fold 3 accuracy : 66.66666666666666\n",
      "Fold - 4 (157, 128, 128, 64, 1) (157,) (17, 128, 128, 64, 1) (17,)\n",
      "Train on 157 samples, validate on 17 samples\n",
      "Epoch 1/100\n",
      "157/157 [==============================] - 8s 51ms/step - loss: 0.5043 - acc: 0.4841 - val_loss: 0.4737 - val_acc: 0.5294\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.4634 - acc: 0.5478 - val_loss: 0.4713 - val_acc: 0.5294\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.3813 - acc: 0.6369 - val_loss: 0.4712 - val_acc: 0.5294\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.3722 - acc: 0.6561 - val_loss: 0.4724 - val_acc: 0.5294\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.3129 - acc: 0.7006 - val_loss: 0.4767 - val_acc: 0.5294\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.2401 - acc: 0.8089 - val_loss: 0.4919 - val_acc: 0.5294\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.2163 - acc: 0.8408 - val_loss: 0.4974 - val_acc: 0.3529\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.2214 - acc: 0.8089 - val_loss: 0.4656 - val_acc: 0.5294\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1976 - acc: 0.8217 - val_loss: 0.4652 - val_acc: 0.5294\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1804 - acc: 0.8344 - val_loss: 0.4673 - val_acc: 0.5294\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1677 - acc: 0.8662 - val_loss: 0.4605 - val_acc: 0.5294\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1115 - acc: 0.9427 - val_loss: 0.4359 - val_acc: 0.5882\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1036 - acc: 0.9363 - val_loss: 0.4375 - val_acc: 0.5882\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1173 - acc: 0.9236 - val_loss: 0.4305 - val_acc: 0.6471\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0913 - acc: 0.9427 - val_loss: 0.4453 - val_acc: 0.5882\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0930 - acc: 0.9363 - val_loss: 0.4407 - val_acc: 0.5882\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0858 - acc: 0.9490 - val_loss: 0.4394 - val_acc: 0.5294\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0804 - acc: 0.9554 - val_loss: 0.4500 - val_acc: 0.4706\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0509 - acc: 0.9873 - val_loss: 0.4464 - val_acc: 0.5294\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0538 - acc: 0.9873 - val_loss: 0.4405 - val_acc: 0.5882\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0515 - acc: 0.9745 - val_loss: 0.4274 - val_acc: 0.5294\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0738 - acc: 0.9554 - val_loss: 0.4303 - val_acc: 0.5882\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0521 - acc: 0.9873 - val_loss: 0.4313 - val_acc: 0.5294\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0475 - acc: 0.9745 - val_loss: 0.4320 - val_acc: 0.5294\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0381 - acc: 0.9809 - val_loss: 0.4389 - val_acc: 0.5882\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0303 - acc: 0.9936 - val_loss: 0.4395 - val_acc: 0.5882\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0384 - acc: 0.9936 - val_loss: 0.4471 - val_acc: 0.5882\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0282 - acc: 1.0000 - val_loss: 0.4391 - val_acc: 0.5294\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0263 - acc: 0.9936 - val_loss: 0.4472 - val_acc: 0.5882\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0297 - acc: 0.9936 - val_loss: 0.4331 - val_acc: 0.5882\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.4170 - val_acc: 0.5882\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0267 - acc: 1.0000 - val_loss: 0.4237 - val_acc: 0.5882\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0267 - acc: 0.9873 - val_loss: 0.4328 - val_acc: 0.5882\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.4313 - val_acc: 0.5882\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.4275 - val_acc: 0.5882\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.4313 - val_acc: 0.5882\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.4391 - val_acc: 0.4706\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.4386 - val_acc: 0.5882\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0188 - acc: 0.9936 - val_loss: 0.4256 - val_acc: 0.6471\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.4251 - val_acc: 0.5882\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0236 - acc: 0.9936 - val_loss: 0.4370 - val_acc: 0.5294\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.4289 - val_acc: 0.5882\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0174 - acc: 0.9936 - val_loss: 0.4236 - val_acc: 0.6471\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.4228 - val_acc: 0.6471\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.4202 - val_acc: 0.5882\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.4180 - val_acc: 0.5882\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.4189 - val_acc: 0.5882\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0176 - acc: 0.9936 - val_loss: 0.4297 - val_acc: 0.5882\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.4309 - val_acc: 0.5882\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.4245 - val_acc: 0.6471\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.4244 - val_acc: 0.6471\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.4299 - val_acc: 0.5882\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.4331 - val_acc: 0.5882\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0160 - acc: 0.9936 - val_loss: 0.4346 - val_acc: 0.5882\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.4193 - val_acc: 0.6471\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.4144 - val_acc: 0.5882\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.4229 - val_acc: 0.5882\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.4295 - val_acc: 0.5882\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.4276 - val_acc: 0.5882\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.4313 - val_acc: 0.5882\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.4359 - val_acc: 0.5882\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.4245 - val_acc: 0.5882\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.4242 - val_acc: 0.5882\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.4236 - val_acc: 0.6471\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.4267 - val_acc: 0.6471\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.4294 - val_acc: 0.5882\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.4366 - val_acc: 0.5294\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.4322 - val_acc: 0.5294\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.4219 - val_acc: 0.5882\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.4160 - val_acc: 0.6471\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.4163 - val_acc: 0.6471\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0082 - acc: 0.9936 - val_loss: 0.4197 - val_acc: 0.6471\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4203 - val_acc: 0.6471\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.4177 - val_acc: 0.6471\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.4164 - val_acc: 0.6471\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.4207 - val_acc: 0.5882\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0111 - acc: 0.9936 - val_loss: 0.4204 - val_acc: 0.5882\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.4195 - val_acc: 0.5882\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.4187 - val_acc: 0.6471\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.4176 - val_acc: 0.6471\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4231 - val_acc: 0.5882\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4274 - val_acc: 0.5882\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.4243 - val_acc: 0.5882\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.4241 - val_acc: 0.5882\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4261 - val_acc: 0.5882\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.4282 - val_acc: 0.5882\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.4312 - val_acc: 0.5882\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.4315 - val_acc: 0.5882\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4288 - val_acc: 0.6471\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.4275 - val_acc: 0.6471\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4258 - val_acc: 0.6471\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.4254 - val_acc: 0.6471\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0060 - acc: 0.9936 - val_loss: 0.4217 - val_acc: 0.5882\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.4235 - val_acc: 0.5882\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.4268 - val_acc: 0.5882\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.4257 - val_acc: 0.5882\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.4230 - val_acc: 0.6471\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.4192 - val_acc: 0.6471\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.4179 - val_acc: 0.6471\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.4196 - val_acc: 0.6471\n",
      "Fold 4 accuracy : 64.70588235294117\n",
      "Fold - 5 (157, 128, 128, 64, 1) (157,) (17, 128, 128, 64, 1) (17,)\n",
      "Train on 157 samples, validate on 17 samples\n",
      "Epoch 1/100\n",
      "157/157 [==============================] - 7s 45ms/step - loss: 0.4894 - acc: 0.4841 - val_loss: 0.4924 - val_acc: 0.5294\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.5053 - acc: 0.5223 - val_loss: 0.4903 - val_acc: 0.5294\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.4326 - acc: 0.5605 - val_loss: 0.4928 - val_acc: 0.5294\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.3526 - acc: 0.6624 - val_loss: 0.4974 - val_acc: 0.4706\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.3070 - acc: 0.7261 - val_loss: 0.5122 - val_acc: 0.4706\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.2714 - acc: 0.7452 - val_loss: 0.5167 - val_acc: 0.4706\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.2721 - acc: 0.7707 - val_loss: 0.5101 - val_acc: 0.4706\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.2168 - acc: 0.8280 - val_loss: 0.4990 - val_acc: 0.4706\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.1954 - acc: 0.8408 - val_loss: 0.4494 - val_acc: 0.5294\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.1638 - acc: 0.8662 - val_loss: 0.4099 - val_acc: 0.5882\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1784 - acc: 0.8408 - val_loss: 0.3669 - val_acc: 0.7647\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.1242 - acc: 0.8981 - val_loss: 0.3820 - val_acc: 0.7059\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.1275 - acc: 0.9045 - val_loss: 0.4067 - val_acc: 0.6471\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.1011 - acc: 0.9299 - val_loss: 0.3955 - val_acc: 0.6471\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.1173 - acc: 0.9108 - val_loss: 0.3674 - val_acc: 0.7059\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0934 - acc: 0.9427 - val_loss: 0.3666 - val_acc: 0.7059\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0847 - acc: 0.9554 - val_loss: 0.3531 - val_acc: 0.6471\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0730 - acc: 0.9427 - val_loss: 0.3627 - val_acc: 0.6471\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0787 - acc: 0.9682 - val_loss: 0.3817 - val_acc: 0.7059\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0639 - acc: 0.9618 - val_loss: 0.3576 - val_acc: 0.6471\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0438 - acc: 0.9873 - val_loss: 0.3706 - val_acc: 0.6471\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0576 - acc: 0.9682 - val_loss: 0.4067 - val_acc: 0.7059\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0425 - acc: 0.9936 - val_loss: 0.3931 - val_acc: 0.7647\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0415 - acc: 0.9809 - val_loss: 0.3821 - val_acc: 0.7059\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0547 - acc: 0.9618 - val_loss: 0.3844 - val_acc: 0.7059\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0293 - acc: 0.9809 - val_loss: 0.3867 - val_acc: 0.7059\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0472 - acc: 0.9618 - val_loss: 0.4009 - val_acc: 0.7059\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.3654 - val_acc: 0.7059\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0368 - acc: 0.9809 - val_loss: 0.3552 - val_acc: 0.6471\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0359 - acc: 0.9873 - val_loss: 0.3682 - val_acc: 0.7059\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0335 - acc: 0.9873 - val_loss: 0.3529 - val_acc: 0.7059\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.3542 - val_acc: 0.7059\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0212 - acc: 1.0000 - val_loss: 0.3614 - val_acc: 0.7059\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0317 - acc: 0.9809 - val_loss: 0.3530 - val_acc: 0.7059\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0303 - acc: 0.9873 - val_loss: 0.3511 - val_acc: 0.7059\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.3796 - val_acc: 0.7647\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0284 - acc: 1.0000 - val_loss: 0.3659 - val_acc: 0.7059\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.3581 - val_acc: 0.6471\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0194 - acc: 0.9873 - val_loss: 0.3508 - val_acc: 0.6471\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0227 - acc: 1.0000 - val_loss: 0.3472 - val_acc: 0.6471\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.3590 - val_acc: 0.7059\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0168 - acc: 0.9936 - val_loss: 0.3671 - val_acc: 0.7059\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.3575 - val_acc: 0.7059\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0135 - acc: 0.9936 - val_loss: 0.3573 - val_acc: 0.7059\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.3601 - val_acc: 0.7059\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0220 - acc: 0.9873 - val_loss: 0.3536 - val_acc: 0.7059\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.3463 - val_acc: 0.7059\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.3532 - val_acc: 0.7059\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.3609 - val_acc: 0.7059\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.3607 - val_acc: 0.7059\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.3601 - val_acc: 0.7059\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.3659 - val_acc: 0.7059\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.3613 - val_acc: 0.7059\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0219 - acc: 0.9936 - val_loss: 0.3581 - val_acc: 0.7059\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.3461 - val_acc: 0.7059\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.3411 - val_acc: 0.7059\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.3307 - val_acc: 0.7059\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.3243 - val_acc: 0.7059\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0123 - acc: 0.9936 - val_loss: 0.3217 - val_acc: 0.7059\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0129 - acc: 0.9936 - val_loss: 0.3178 - val_acc: 0.7647\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.3227 - val_acc: 0.7059\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.3369 - val_acc: 0.7059\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0171 - acc: 0.9936 - val_loss: 0.3343 - val_acc: 0.7059\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0128 - acc: 0.9936 - val_loss: 0.3378 - val_acc: 0.7059\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.3387 - val_acc: 0.7059\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.3346 - val_acc: 0.7059\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.3358 - val_acc: 0.7059\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0124 - acc: 0.9936 - val_loss: 0.3371 - val_acc: 0.7059\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.3322 - val_acc: 0.7059\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.3307 - val_acc: 0.7059\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.3430 - val_acc: 0.7059\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.3488 - val_acc: 0.7059\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.3509 - val_acc: 0.7059\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0112 - acc: 0.9936 - val_loss: 0.3447 - val_acc: 0.7059\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.3334 - val_acc: 0.7059\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.3210 - val_acc: 0.7059\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 0.7059\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.3244 - val_acc: 0.7059\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.3349 - val_acc: 0.7059\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.3400 - val_acc: 0.7059\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3427 - val_acc: 0.7059\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.3410 - val_acc: 0.7059\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.3391 - val_acc: 0.7059\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.3341 - val_acc: 0.7059\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.3262 - val_acc: 0.7059\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.3277 - val_acc: 0.7059\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.3380 - val_acc: 0.7059\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3419 - val_acc: 0.7059\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.3404 - val_acc: 0.7059\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.3360 - val_acc: 0.7059\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.3345 - val_acc: 0.7059\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.3344 - val_acc: 0.7059\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.3327 - val_acc: 0.7059\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0113 - acc: 0.9936 - val_loss: 0.3311 - val_acc: 0.7059\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.3362 - val_acc: 0.7059\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.3394 - val_acc: 0.7059\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.3385 - val_acc: 0.7059\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.3407 - val_acc: 0.7059\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.3349 - val_acc: 0.7059\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.3341 - val_acc: 0.7059\n",
      "Fold 5 accuracy : 70.58823529411765\n",
      "Fold - 6 (157, 128, 128, 64, 1) (157,) (17, 128, 128, 64, 1) (17,)\n",
      "Train on 157 samples, validate on 17 samples\n",
      "Epoch 1/100\n",
      "157/157 [==============================] - 7s 45ms/step - loss: 0.5003 - acc: 0.4904 - val_loss: 0.5226 - val_acc: 0.4706\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.4605 - acc: 0.5669 - val_loss: 0.5282 - val_acc: 0.4706\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.4249 - acc: 0.5732 - val_loss: 0.5292 - val_acc: 0.4706\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.3503 - acc: 0.6879 - val_loss: 0.5294 - val_acc: 0.4706\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.2885 - acc: 0.7389 - val_loss: 0.5294 - val_acc: 0.4706\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.2880 - acc: 0.7452 - val_loss: 0.5294 - val_acc: 0.4706\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.2554 - acc: 0.7834 - val_loss: 0.5297 - val_acc: 0.4706\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1828 - acc: 0.8599 - val_loss: 0.5274 - val_acc: 0.4706\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.2073 - acc: 0.8280 - val_loss: 0.4856 - val_acc: 0.5294\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.1642 - acc: 0.8662 - val_loss: 0.4668 - val_acc: 0.5294\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1543 - acc: 0.8726 - val_loss: 0.4727 - val_acc: 0.5294\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1138 - acc: 0.9363 - val_loss: 0.4577 - val_acc: 0.5882\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1276 - acc: 0.9172 - val_loss: 0.4535 - val_acc: 0.5294\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.1136 - acc: 0.9236 - val_loss: 0.4322 - val_acc: 0.5882\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0765 - acc: 0.9554 - val_loss: 0.4330 - val_acc: 0.5882\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0915 - acc: 0.9490 - val_loss: 0.4460 - val_acc: 0.5882\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0905 - acc: 0.9618 - val_loss: 0.4483 - val_acc: 0.5294\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0690 - acc: 0.9554 - val_loss: 0.4410 - val_acc: 0.5882\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0726 - acc: 0.9618 - val_loss: 0.4518 - val_acc: 0.5294\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0487 - acc: 0.9745 - val_loss: 0.4488 - val_acc: 0.5882\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0406 - acc: 0.9809 - val_loss: 0.4481 - val_acc: 0.5294\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0550 - acc: 0.9682 - val_loss: 0.4497 - val_acc: 0.5294\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0543 - acc: 0.9745 - val_loss: 0.4454 - val_acc: 0.5294\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0458 - acc: 0.9745 - val_loss: 0.4423 - val_acc: 0.5882\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0429 - acc: 0.9873 - val_loss: 0.4345 - val_acc: 0.5882\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0241 - acc: 1.0000 - val_loss: 0.4370 - val_acc: 0.5882\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0311 - acc: 0.9873 - val_loss: 0.4270 - val_acc: 0.6471\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0274 - acc: 0.9936 - val_loss: 0.4221 - val_acc: 0.5882\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0282 - acc: 0.9809 - val_loss: 0.4212 - val_acc: 0.5882\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0270 - acc: 0.9936 - val_loss: 0.4393 - val_acc: 0.5294\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0224 - acc: 0.9873 - val_loss: 0.4395 - val_acc: 0.5882\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.4459 - val_acc: 0.4706\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0282 - acc: 0.9873 - val_loss: 0.4451 - val_acc: 0.5294\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.4527 - val_acc: 0.5294\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.4620 - val_acc: 0.5294\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0161 - acc: 0.9936 - val_loss: 0.4680 - val_acc: 0.4706\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.4663 - val_acc: 0.4118\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.4611 - val_acc: 0.4118\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.4628 - val_acc: 0.4706\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.4559 - val_acc: 0.5882\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.4511 - val_acc: 0.5294\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.4522 - val_acc: 0.5882\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.4519 - val_acc: 0.4706\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.4573 - val_acc: 0.5294\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0191 - acc: 0.9936 - val_loss: 0.4553 - val_acc: 0.5294\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.4567 - val_acc: 0.5294\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.4598 - val_acc: 0.5294\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.4549 - val_acc: 0.4706\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.4483 - val_acc: 0.4706\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.4443 - val_acc: 0.5882\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.4401 - val_acc: 0.5882\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.4420 - val_acc: 0.5882\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.4445 - val_acc: 0.5882\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.4469 - val_acc: 0.5882\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.4477 - val_acc: 0.5294\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.4455 - val_acc: 0.5294\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.4481 - val_acc: 0.5294\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.4494 - val_acc: 0.5294\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.4454 - val_acc: 0.5294\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.4407 - val_acc: 0.5294\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.4362 - val_acc: 0.4706\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.4363 - val_acc: 0.4706\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.4397 - val_acc: 0.5294\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0126 - acc: 0.9936 - val_loss: 0.4445 - val_acc: 0.5882\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.4477 - val_acc: 0.6471\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.4477 - val_acc: 0.5294\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.4413 - val_acc: 0.5294\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.4394 - val_acc: 0.5882\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.4396 - val_acc: 0.5294\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.4401 - val_acc: 0.5294\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.4451 - val_acc: 0.5294\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0109 - acc: 0.9936 - val_loss: 0.4453 - val_acc: 0.5882\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.4433 - val_acc: 0.5882\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 7s 41ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.4422 - val_acc: 0.5882\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.4389 - val_acc: 0.6471\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.4376 - val_acc: 0.5882\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.4399 - val_acc: 0.5882\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.4429 - val_acc: 0.5882\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.4428 - val_acc: 0.5882\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.4406 - val_acc: 0.5882\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.4417 - val_acc: 0.5294\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.4420 - val_acc: 0.5882\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0085 - acc: 0.9936 - val_loss: 0.4379 - val_acc: 0.5294\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 7s 43ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.4355 - val_acc: 0.5294\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.4368 - val_acc: 0.4706\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.4389 - val_acc: 0.4706\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 6s 41ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.4452 - val_acc: 0.5294\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 6s 39ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.4471 - val_acc: 0.5294\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 6s 40ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.4486 - val_acc: 0.5882\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 6s 39ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.4498 - val_acc: 0.5294\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 6s 39ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.4463 - val_acc: 0.5294\n",
      "Epoch 92/100\n",
      " 50/157 [========>.....................] - ETA: 4s - loss: 0.0047 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "# kfold cross validation\n",
    "# https://www.kaggle.com/sharifamit19/data-augmentation-cross-validation-ensemble\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# Calculate the starting time    \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# define k-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "print(kfold)\n",
    "\n",
    "\n",
    "data = X_train\n",
    "# Flatten labels for making folds\n",
    "labels = np.array([np.argmax(x) for x in y_train])\n",
    "print(data.shape, labels.shape)\n",
    "\n",
    "\n",
    "# Store scores here\n",
    "cross_val_scores = []\n",
    "\n",
    "# Run folds\n",
    "for i, (train, test) in enumerate(kfold.split(data, labels)):\n",
    "    \n",
    "    print(\"Fold - {}\".format(i), data[train].shape, labels[train].shape, data[test].shape, labels[test].shape)\n",
    "    \n",
    "    # Clearing the NN.\n",
    "    K.clear_session()\n",
    "    model = None \n",
    "    \n",
    "    # Calculate the starting time    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create the model\n",
    "    model = awesome_3D_network()\n",
    "    \n",
    "    # One hot :convert class vectors to binary class matrices\n",
    "    y_train_cv = keras.utils.to_categorical(labels[train], 2)\n",
    "    y_val_cv = keras.utils.to_categorical(labels[test], 2)\n",
    "    \n",
    "    # Set callbacks\n",
    "    cb = None #[csv_logger, early_stopping, reduce_lr, checkpointer]\n",
    "    cw = None #class_weights\n",
    "\n",
    "    # Train\n",
    "    h=model.fit(x=data[train],     \n",
    "                y=y_train_cv,\n",
    "                validation_data=(data[test], y_val_cv), \n",
    "                batch_size=2, \n",
    "                epochs=100, \n",
    "                verbose=1,\n",
    "                class_weight = cw,\n",
    "                callbacks=cb,\n",
    "                shuffle=False,\n",
    "                )\n",
    "\n",
    "    # Evaluate\n",
    "\n",
    "    #score = model.evaluate(data[test], y_val_cv, verbose=0)\n",
    "    #loss, acc = score[0], score[1]\n",
    "\n",
    "    # Validation predictions stored here\n",
    "    y_pred_cv = []\n",
    "\n",
    "    # Iterate over images in validation\n",
    "    for img in data[test]:\n",
    "        img = np.expand_dims(img, axis=0)  # rank 4 tensor for prediction\n",
    "        y = model.predict(img)\n",
    "        y_pred_cv.append(y[:][0])\n",
    "\n",
    "    # Numpy\n",
    "    y_pred_cv = np.array(y_pred_cv)\n",
    "    \n",
    "    # Convert ground truth to column values\n",
    "    y_val_cv_flat = np.argmax(y_val_cv, axis=1)\n",
    "    # Get labels from predictions\n",
    "    y_pred_cv_flat = np.array([np.argmax(pred) for pred in y_pred_cv]) # y_pred[1] -> probability for class 1 \n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_val_cv_flat, y_pred_cv_flat) * 100\n",
    "    print(\"Fold {} accuracy :\".format(i), acc)\n",
    "\n",
    "    cross_val_scores.append(acc)\n",
    "\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(start_time, end_time)\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpGKr9eu-e0M"
   },
   "outputs": [],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cross_val_scores), np.std(cross_val_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vYDs1AhB4u8"
   },
   "outputs": [],
   "source": [
    "#SIZ: 61.96% (+/- 10.12%)\n",
    "#SSS: \n",
    "#ESS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPox7VRvB4zd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7KmWAqmtp-N"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "cb = None #[csv_logger, early_stopping, reduce_lr, checkpointer]\n",
    "cw = None #class_weights\n",
    "\n",
    "# Calculate the starting time    \n",
    "start_time = time.time()\n",
    "\n",
    "# One hot :convert class vectors to binary class matrices\n",
    "y_train_cv = keras.utils.to_categorical(y_train, 2)\n",
    "y_val_cv = keras.utils.to_categorical(y_val, 2)\n",
    "\n",
    "# Train\n",
    "h=model.fit(x=X_train,     \n",
    "            y=y_train,\n",
    "            validation_data=(X_val, y_val), \n",
    "            batch_size=2, \n",
    "            epochs=100, \n",
    "            verbose=1,\n",
    "            class_weight = cw,\n",
    "            callbacks=cb,\n",
    "            shuffle=False,\n",
    "            )\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bC3nKRDxUMm8"
   },
   "outputs": [],
   "source": [
    "# Plot and save accuravy loss graphs together\n",
    "def plot_loss_accu_all(history):\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'r')\n",
    "    plt.plot(epochs, val_acc, 'b')\n",
    "    plt.plot(epochs, loss, 'g')\n",
    "    plt.plot(epochs, val_loss, 'y')\n",
    "    plt.title('Accuracy/Loss graph')\n",
    "    \n",
    "    plt.ylabel('Performance Measure')\n",
    "    plt.xlabel('Epochs')\n",
    "    \n",
    "    plt.legend(['trainacc', 'valacc', 'trainloss', 'valloss'], loc='lower right', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('{}/{}.png'.format(log_path+\"/\"+experiment_name, \"trainval_acc_loss\"), dpi=500)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot and save accuravy loss graphs individually\n",
    "def plot_loss_accu(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'g')\n",
    "    plt.plot(epochs, val_loss, 'y')\n",
    "    #plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss %')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig('{}/{}_loss.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    #plt.savefig('{}/{}_loss.pdf'.format(output_path, EXP_NAME), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    loss = history.history['acc']\n",
    "    val_loss = history.history['val_acc']\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'r')\n",
    "    plt.plot(epochs, val_loss, 'b')\n",
    "    #plt.title('Training and validation accuracy')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig('{}/{}_acc.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    #plt.savefig('{}/{}_acc.pdf'.format(output_path, EXP_NAME), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_accu_all(model.history)\n",
    "print(\"Done training and logging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17qsdZzX3qcB"
   },
   "outputs": [],
   "source": [
    "model.save(\"{}/{}.h5\".format(log_path+\"/\"+experiment_name, experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGYJW-hwTUZw"
   },
   "outputs": [],
   "source": [
    "#model = None\n",
    "#model = load_model(\"{}/best_model.h5\".format(model_path))\n",
    "\n",
    "#score = model.evaluate(X_test, y_test, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YECSNf0vtTj"
   },
   "outputs": [],
   "source": [
    "# Inference on hold out test set\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for img in X_test:\n",
    "    img = np.expand_dims(img, axis=0)  # rank 4 tensor for prediction\n",
    "    y = model.predict(img)\n",
    "    y_pred.append(y[:][0])\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc9BbiRXTUdL"
   },
   "outputs": [],
   "source": [
    "# Convert ground truth to column values\n",
    "y_test_flat = np.argmax(y_test, axis=1)\n",
    "print(\"After flattening ground truth: \", y_test_flat.shape)\n",
    "\n",
    "# Get labels from predictions\n",
    "y_pred_flat = np.array([np.argmax(pred) for pred in y_pred]) # y_pred[1] -> probability for class 1 \n",
    "print(\"Binarize probability values: \", y_pred_flat.shape)\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "acc = accuracy_score(y_test_flat, y_pred_flat) * 100\n",
    "print(\"Accuracy :\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIZ: 70.45\n",
    "# SSS: \n",
    "# ESS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64nwLFXbw2Vm"
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test_flat, y_pred_flat) \n",
    "print(confusion_mtx)\n",
    "target_names = ['0', '1']\n",
    "print(classification_report(y_test_flat, y_pred_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[[16  8]\\n [ 5 15]]\\n              precision    recall  f1-score   support\\n\\n           0       0.76      0.67      0.71        24\\n           1       0.65      0.75      0.70        20\\n\\n    accuracy                           0.70        44\\n   macro avg       0.71      0.71      0.70        44\\nweighted avg       0.71      0.70      0.71        44\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SIZ\n",
    "'''\n",
    "[[16  8]\n",
    " [ 5 15]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.76      0.67      0.71        24\n",
    "           1       0.65      0.75      0.70        20\n",
    "\n",
    "    accuracy                           0.70        44\n",
    "   macro avg       0.71      0.71      0.70        44\n",
    "weighted avg       0.71      0.70      0.71        44\n",
    "\n",
    "'''\n",
    "\n",
    "#SSS\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEPVB-Qqw2Pe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve :  73.125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Area under ROC curve : ', roc_auc_score(y_test, y_pred) *100 )\n",
    "\n",
    "# sss \n",
    "# ess \n",
    "# siz 73.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Hh61ucpx6_c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for positive class: 0.7312500000000001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "cls = 1 # class name\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#print(roc_auc)\n",
    "print(\"Area under the ROC curve for positive class:\", roc_auc[1])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2 # line width\n",
    "plt.plot(fpr[cls], tpr[cls], color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[cls])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('{}/{}.png'.format(log_path+\"/\"+experiment_name, \"roc\"), dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibMnW_4kbhpy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, 0.7312500000000001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helpers\n",
    "import pickle \n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('{}'.format(log_path+\"/\") + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('{}'.format(log_path+\"/\") + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "#https://github.com/hasibzunair/MelaNet/blob/master/isic2016_scripts/EDA.ipynb\n",
    "\n",
    "# Save AUCROC for plotting\n",
    "ascore = {}\n",
    "ascore[\"fpr\"] = fpr[cls]\n",
    "ascore[\"tpr\"] = tpr[cls]\n",
    "ascore[\"roc_auc\"] = roc_auc[cls]\n",
    "save_obj(ascore, experiment_name)\n",
    "\n",
    "type(fpr[cls]), roc_auc[cls] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TL4iR6rdVox0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ut7rFRxVor8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCt7UgGMVoqE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crSuFNoYVonr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqN510l6TUgK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsBAJmom31Ep"
   },
   "source": [
    "### Inference on ImageCLEF test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Syi4HYH73qfX"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model = load_model(\"{}/best_model.h5\".format(model_path))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F60ZqaHO3qnP"
   },
   "outputs": [],
   "source": [
    "x_test = expand_dims(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlYy17EB3qtG"
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in x_test:\n",
    "    i = np.expand_dims(i, axis=0)\n",
    "    y_pred = model.predict(i)\n",
    "    res.append(y_pred)\n",
    "    #print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2uoQDJK3qq9"
   },
   "outputs": [],
   "source": [
    "res = np.array(res)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWtc_No1iI7p"
   },
   "outputs": [],
   "source": [
    "res[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95h2C3JlidGr"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcoXaust3qkz"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "dt = pd.read_csv('{}/TestSet_metaData.csv'.format(dataset_path))\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPBhkjsM3qiy"
   },
   "outputs": [],
   "source": [
    "patient_names = dt['Filename'].values\n",
    "len(patient_names)\n",
    "\n",
    "names = []\n",
    "\n",
    "for name in patient_names:\n",
    "    names.append(name[:-7])\n",
    "\n",
    "names[:5]\n",
    "\n",
    "probab = []\n",
    "\n",
    "for p in res:\n",
    "    # probability of HIGH severity as required to make submission\n",
    "    probab.append(p[0][1])\n",
    "    \n",
    "probab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7oyQ2uMD3qY-"
   },
   "outputs": [],
   "source": [
    "for n, p in zip(names, probab):\n",
    "    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qE_cR4n4tqMn"
   },
   "outputs": [],
   "source": [
    "with open('{}/submission.txt'.format(dataset_path), 'w') as f:\n",
    "    for n, p in zip(names, probab):\n",
    "        print(n,\",\", p)\n",
    "        f.write(str(n))\n",
    "        f.write(\",\")\n",
    "        f.write(str(p))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMDeNXJx1TxkuBCBZX5hqeg",
   "collapsed_sections": [],
   "name": "train_clef19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
